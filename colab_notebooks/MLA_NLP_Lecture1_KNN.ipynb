{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLA-NLP-Lecture1-KNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxhPFf6J124k4IFYPJyYes",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LxYuan0420/aws-machine-learning-university-accelerated-nlp/blob/master/colab_notebooks/MLA_NLP_Lecture1_KNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afkPVzupq5jn",
        "outputId": "f444b2b4-212a-44eb-c142-37a08c2713f9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cCX3BvkZq5jp",
        "outputId": "f71becda-cb27-4775-ac5b-4cb1a1287435"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq5pS8z0rBir",
        "outputId": "86577f72-a72c-40d9-b445-1f2553d26719"
      },
      "source": [
        "%cd /gdrive/MyDrive/Colab Notebooks/git/aws-machine-learning-university-accelerated-nlp/colab_notebooks"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/Colab Notebooks/git/aws-machine-learning-university-accelerated-nlp/colab_notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuK5gdglqSu8"
      },
      "source": [
        "**Machine Learning Accelerator - Natural Language Processing - Lecture 1**\r\n",
        "\r\n",
        "K Nearest Neighbors Model for a Classification Problem: Classify Product Reviews as Positive or Negative\r\n",
        "In this notebook, we use the K Nearest Neighbors method to build a classifier to predict the isPositive field of our review dataset (that is very similar to the final project dataset).\r\n",
        "\r\n",
        "1. Reading the dataset\r\n",
        "2. Exploratory data analysis\r\n",
        "3. Text Processing: Stop words removal and stemming\r\n",
        "4. Train - Validation Split\r\n",
        "5. Data processing with Pipeline\r\n",
        "6. Train the classifier\r\n",
        "7. Test the classifier Find more details on the KNN Classifier here: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\r\n",
        "8. Ideas for improvement\r\n",
        "\r\n",
        "Overall dataset schema:\r\n",
        "\r\n",
        "- reviewText: Text of the review\r\n",
        "- summary: Summary of the review\r\n",
        "- verified: Whether the purchase was verified (True or False)\r\n",
        "- time: UNIX timestamp for the review\r\n",
        "- log_votes: Logarithm-adjusted votes log(1+votes). This field is a processed version of the votes field. People can click on the \"helpful\" button when they find a customer review helpful. This increases the vote by 1. log_votes is calculated like this log(1+votes). This formulation helps us get a smaller range for votes.\r\n",
        "- isPositive: Whether the review is positive or negative (1 or 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-OYepzdqnWU"
      },
      "source": [
        "**1. Reading dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bgse96tJrNVs",
        "outputId": "ab711667-de24-4def-bec3-7c622e586be0"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "df = pd.read_csv(\"../data/examples/AMAZON-REVIEW-DATA-CLASSIFICATION.csv\")\r\n",
        "\r\n",
        "print(f\"The shape of the dataset is {df.shape}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the dataset is (70000, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "FJKYJ_VIrbrO",
        "outputId": "9bb417e3-6562-4b22-be59-ab48dfe36014"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewText</th>\n",
              "      <th>summary</th>\n",
              "      <th>verified</th>\n",
              "      <th>time</th>\n",
              "      <th>log_votes</th>\n",
              "      <th>isPositive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PURCHASED FOR YOUNGSTER WHO\\nINHERITED MY \"TOO...</td>\n",
              "      <td>IDEAL FOR BEGINNER!</td>\n",
              "      <td>True</td>\n",
              "      <td>1361836800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>unable to open or use</td>\n",
              "      <td>Two Stars</td>\n",
              "      <td>True</td>\n",
              "      <td>1452643200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Waste of money!!! It wouldn't load to my system.</td>\n",
              "      <td>Dont buy it!</td>\n",
              "      <td>True</td>\n",
              "      <td>1433289600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I attempted to install this OS on two differen...</td>\n",
              "      <td>I attempted to install this OS on two differen...</td>\n",
              "      <td>True</td>\n",
              "      <td>1518912000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I've spent 14 fruitless hours over the past tw...</td>\n",
              "      <td>Do NOT Download.</td>\n",
              "      <td>True</td>\n",
              "      <td>1441929600</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          reviewText  ... isPositive\n",
              "0  PURCHASED FOR YOUNGSTER WHO\\nINHERITED MY \"TOO...  ...        1.0\n",
              "1                              unable to open or use  ...        0.0\n",
              "2   Waste of money!!! It wouldn't load to my system.  ...        0.0\n",
              "3  I attempted to install this OS on two differen...  ...        0.0\n",
              "4  I've spent 14 fruitless hours over the past tw...  ...        0.0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEb5hqxnrfGc"
      },
      "source": [
        "**2. Exploratory data analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9TfpWoSrifr",
        "outputId": "b7d57f6d-5acd-479e-a2e1-f9dbe2bbee65"
      },
      "source": [
        "df['isPositive'].value_counts()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0    43692\n",
              "0.0    26308\n",
              "Name: isPositive, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQdnRdbWrn28",
        "outputId": "bd74f6e6-83e5-40db-f795-135ad5d90a44"
      },
      "source": [
        "print(df.isna().sum())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reviewText    11\n",
            "summary       14\n",
            "verified       0\n",
            "time           0\n",
            "log_votes      0\n",
            "isPositive     0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-bgcpBYrtV0"
      },
      "source": [
        "**3. Text Processing: Stop words removal and steamming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8zBlLuUsZWL",
        "outputId": "53078313-46ee-43e3-fe34-ebae251673f2"
      },
      "source": [
        "import nltk\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6v954_lseMM"
      },
      "source": [
        "import nltk, re\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import SnowballStemmer\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "# Let's get a list of stop words from the NLTK library\r\n",
        "stop = stopwords.words('english')\r\n",
        "\r\n",
        "# These words are important for our problem. We don't want to remove them.\r\n",
        "excluding = ['against', 'not', 'don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\r\n",
        "             'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", \r\n",
        "             'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\",\r\n",
        "             'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \r\n",
        "             \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\r\n",
        "\r\n",
        "# New stop word list\r\n",
        "stop_words = [word for word in stop if word not in excluding]\r\n",
        "\r\n",
        "snow = SnowballStemmer('english')\r\n",
        "\r\n",
        "def process_text(texts):\r\n",
        "    final_text_list = []\r\n",
        "\r\n",
        "    for sent in texts:\r\n",
        "\r\n",
        "        if isinstance(sent, str) == False:\r\n",
        "            sent = \"\"\r\n",
        "\r\n",
        "        filtered_sentence = []\r\n",
        "\r\n",
        "        sent = sent.lower()\r\n",
        "        sent = sent.strip()\r\n",
        "        sent = re.sub(\"\\s+\", \"\", sent) \r\n",
        "        sent = re.compile(\"<.*?>\").sub(\"\", sent)\r\n",
        "\r\n",
        "        for w in word_tokenize(sent):\r\n",
        "            if (not w.isnumeric()) and (len(w)>2) and (w not in stop_words):\r\n",
        "                filtered_sentence.append(w)\r\n",
        "        final_string = \" \".join(filtered_sentence)\r\n",
        "\r\n",
        "        final_text_list.append(final_string)\r\n",
        "\r\n",
        "    return final_text_list\r\n",
        "\r\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-Hf1nQVt-B3"
      },
      "source": [
        "**Train - validation Split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi7UsjEGuA1z"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(df[[\"reviewText\"]],\r\n",
        "                                                  df[\"isPositive\"],\r\n",
        "                                                  test_size =0.10,\r\n",
        "                                                  shuffle=True,\r\n",
        "                                                  random_state=324)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaMgLLEQuTMd",
        "outputId": "1e297121-1b21-4ca2-c115-9f5f58b5cc6c"
      },
      "source": [
        "print(\"Processing the reviewText fields\")\r\n",
        "train_text_list = process_text(X_train[\"reviewText\"].tolist())\r\n",
        "val_text_list = process_text(X_val[\"reviewText\"].tolist())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing the reviewText fields\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXwmeMWmuh38"
      },
      "source": [
        "**5. Data processing with Pipeline**\r\n",
        "\r\n",
        "(Go to top)\r\n",
        "\r\n",
        "Today we will use a simple pipeline to use our text field and fit a simple K Nearest Neighbors classifier. This example only uses a single field (reviewText). In the next lecture, we will see how to combine multiple fields.\r\n",
        "\r\n",
        "Our CountVectorizer() will return binary values and use 15 vocabulary words. Feel free to experiment with different numbers here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7-d4zOiuoLc",
        "outputId": "83b19f16-0f0b-4e45-c8a2-cbe3771bdf50"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "\r\n",
        "pipeline = Pipeline([\r\n",
        "    (\"text_vect\", CountVectorizer(binary=True, max_features=15)),\r\n",
        "    (\"knn\", KNeighborsClassifier())\r\n",
        "])\r\n",
        "\r\n",
        "#Visualize the pipline\r\n",
        "pipeline"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('text_vect',\n",
              "                 CountVectorizer(analyzer='word', binary=True,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=15, min_df=1, ngram_range=(1, 1),\n",
              "                                 preprocessor=None, stop_words=None,\n",
              "                                 strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('knn',\n",
              "                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
              "                                      metric='minkowski', metric_params=None,\n",
              "                                      n_jobs=None, n_neighbors=5, p=2,\n",
              "                                      weights='uniform'))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMHhqrymvCQO"
      },
      "source": [
        "**Train the classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t467aB3YvPvc",
        "outputId": "58692815-799c-45f4-fab6-9cc9bfa5498c"
      },
      "source": [
        "X_train = train_text_list\r\n",
        "X_val = val_text_list\r\n",
        "\r\n",
        "\r\n",
        "pipeline.fit(X_train, y_train.values)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('text_vect',\n",
              "                 CountVectorizer(analyzer='word', binary=True,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=15, min_df=1, ngram_range=(1, 1),\n",
              "                                 preprocessor=None, stop_words=None,\n",
              "                                 strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('knn',\n",
              "                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
              "                                      metric='minkowski', metric_params=None,\n",
              "                                      n_jobs=None, n_neighbors=5, p=2,\n",
              "                                      weights='uniform'))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IXXJntrvXz1"
      },
      "source": [
        "**Test classififer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ef1j-7WIvjSt",
        "outputId": "e277203c-87a2-4494-e6ab-febcb0cfb07f"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\r\n",
        "\r\n",
        "val_preds = pipeline.predict(X_val)\r\n",
        "print(confusion_matrix(y_val.values, val_preds))\r\n",
        "print(classification_report(y_val.values, val_preds))\r\n",
        "print(f\"Accuracy (validation): {accuracy_score(y_val.values, val_preds)}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 493 2112]\n",
            " [ 416 3979]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.54      0.19      0.28      2605\n",
            "         1.0       0.65      0.91      0.76      4395\n",
            "\n",
            "    accuracy                           0.64      7000\n",
            "   macro avg       0.60      0.55      0.52      7000\n",
            "weighted avg       0.61      0.64      0.58      7000\n",
            "\n",
            "Accuracy (validation): 0.6388571428571429\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}