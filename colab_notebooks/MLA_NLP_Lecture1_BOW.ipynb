{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLA-NLP-Lecture1-BOW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMn7z0vsRykEBJIrQ+Zlao/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LxYuan0420/aws-machine-learning-university-accelerated-nlp/blob/master/colab_notebooks/MLA_NLP_Lecture1_BOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBGtCNoNn7Qk"
      },
      "source": [
        "**Bag of Words Method**\r\n",
        "\r\n",
        "In this notebook, we go over the Bag of Words (BoW) method to convert text data into numerical values, that will be later used for predictions with machine learning algorithms.\r\n",
        "\r\n",
        "To convert text data to vectors of numbers, a vocabulary of known words (tokens) is extracted from the text, the occurence of words is scored, and the resulting numerical values are saved in vocabulary-long vectors. There are a few versions of BoW, corresponding to different words scoring methods. We use the Sklearn library to calculate the BoW numerical values using:\r\n",
        "\r\n",
        "1. Binary\r\n",
        "2. Word Counts\r\n",
        "3. Term Frequencies\r\n",
        "4. Term Frequency-Inverse Document Frequencies\r\n",
        "\r\n",
        "\r\n",
        "**1. Binary**\r\n",
        "\r\n",
        "Let's calculate the first type of BoW, recording whether the word is in the sentence or not. We will also go over some useful features of Sklearn's vectorizers here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9c_RwXboIsO"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "\r\n",
        "sentences = [\"This document is the first document\",\r\n",
        "             \"This document is the second document\",\r\n",
        "             \"and this is the third one\"]\r\n",
        "\r\n",
        "bow_vectorizer = CountVectorizer(binary=True)\r\n",
        "\r\n",
        "x = bow_vectorizer.fit_transform(sentences)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTCppQSnokiS",
        "outputId": "4b7f96ba-0ff3-4af7-978f-9b7d7419334e"
      },
      "source": [
        "x.toarray()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
              "       [0, 1, 0, 1, 0, 1, 1, 0, 1],\n",
              "       [1, 0, 0, 1, 1, 0, 1, 1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCHKJojPomxm",
        "outputId": "e6011c1a-22c4-4424-f2c2-3d2a5384fca0"
      },
      "source": [
        "# check out vocoabulary\r\n",
        "bow_vectorizer.vocabulary_"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 0,\n",
              " 'document': 1,\n",
              " 'first': 2,\n",
              " 'is': 3,\n",
              " 'one': 4,\n",
              " 'second': 5,\n",
              " 'the': 6,\n",
              " 'third': 7,\n",
              " 'this': 8}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4buArtY0orvt",
        "outputId": "3751082a-6791-4316-f0b7-ee25cf8d9274"
      },
      "source": [
        "print(f\"There are {len(bow_vectorizer.vocabulary_)} vocabulary so each sentence is transformed to a vector of {len(bow_vectorizer.vocabulary_)}-dimenstion \")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 9 vocabulary so each sentence is transformed to a vector of 9-dimenstion \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZJ4W0dgpEsm",
        "outputId": "47540748-93fa-4c94-b177-b6dc31c5db9f"
      },
      "source": [
        "print(bow_vectorizer.get_feature_names())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqmCiERapJfW",
        "outputId": "14a26f85-304c-4e51-95f0-fb222c795636"
      },
      "source": [
        "new_sentences = [\"This is a new sentence written by Donald Trump.\",\r\n",
        "                 \"Another short sentences\"]\r\n",
        "\r\n",
        "\r\n",
        "new_vector = bow_vectorizer.transform(new_sentences)\r\n",
        "\r\n",
        "new_vector.toarray()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54i7swIzpWVG"
      },
      "source": [
        "**2. Word Counts**\r\n",
        "\r\n",
        "Word counts can be simply calculated using the same CountVectorizer() function without the binary parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYu3u-1cphQ9",
        "outputId": "93785fb0-1016-43c7-c5ba-7463173661cb"
      },
      "source": [
        "sentences = [\"This document is the first document\", \"This document is the second document\", \"and this is the third one\"]\r\n",
        "\r\n",
        "# Initialize the count vectorizer\r\n",
        "count_vectorizer = CountVectorizer()\r\n",
        "\r\n",
        "xc = count_vectorizer.fit_transform(sentences)\r\n",
        "\r\n",
        "xc.toarray()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 2, 1, 1, 0, 0, 1, 0, 1],\n",
              "       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
              "       [1, 0, 0, 1, 1, 0, 1, 1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnrXCeHDppcm",
        "outputId": "d12d5647-d8f1-431e-bf65-f0868933c4d3"
      },
      "source": [
        "count_vectorizer.vocabulary_"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 0,\n",
              " 'document': 1,\n",
              " 'first': 2,\n",
              " 'is': 3,\n",
              " 'one': 4,\n",
              " 'second': 5,\n",
              " 'the': 6,\n",
              " 'third': 7,\n",
              " 'this': 8}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTnhiWVopmTO",
        "outputId": "0e45c49b-8870-477d-bb1a-ac00caa54cbc"
      },
      "source": [
        "new_sentence = [\"This is the new sentence\"]\r\n",
        "new_vectors = count_vectorizer.transform(new_sentence)\r\n",
        "new_vectors.toarray()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 1, 0, 0, 1, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD9IGy8zp4Ud"
      },
      "source": [
        "**3. Term Frequency (TF)**\r\n",
        "\r\n",
        "Term Frequency (TF) vectors that show how important words are to the documents, are computed using\r\n",
        "\r\n",
        "$$tf(term, doc) = \\frac{number\\, of\\, times\\, the\\, term\\, occurs\\, in\\, the\\, doc}{total\\, number\\, of\\, terms\\, in\\, the\\, doc}$$\r\n",
        "From sklearn we use the TfidfVectorizer() function with the parameter use_idf=False, which additionally automatically normalizes the term frequencies vectors by their Euclidean ($l2$) norm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2p9x2BZp7c1",
        "outputId": "e272fba7-9607-4838-9e69-8f0354ce3cce"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "tf_vectorizer = TfidfVectorizer(use_idf=False)\r\n",
        "\r\n",
        "x = tf_vectorizer.fit_transform(sentences)\r\n",
        "\r\n",
        "x.toarray()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.70710678, 0.35355339, 0.35355339, 0.        ,\n",
              "        0.        , 0.35355339, 0.        , 0.35355339],\n",
              "       [0.        , 0.70710678, 0.        , 0.35355339, 0.        ,\n",
              "        0.35355339, 0.35355339, 0.        , 0.35355339],\n",
              "       [0.40824829, 0.        , 0.        , 0.40824829, 0.40824829,\n",
              "        0.        , 0.40824829, 0.40824829, 0.40824829]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paUcAkj9qKwY",
        "outputId": "cddc630d-6ff0-413e-a954-8dfd90d11c93"
      },
      "source": [
        "new_sentence = [\"This is the new sentence\"]\r\n",
        "new_vectors = tf_vectorizer.transform(new_sentence)\r\n",
        "new_vectors.toarray()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.57735027, 0.        ,\n",
              "        0.        , 0.57735027, 0.        , 0.57735027]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_1EhGEbqMFA"
      },
      "source": [
        "**4. Term Frequency Inverse Document Frequency (TF-IDF)**\r\n",
        "\r\n",
        "Term Frequency Inverse Document Frequency (TF-IDF) vectors are computed using the TfidfVectorizer() function with the parameter use_idf=True. We can also skip this parameter as it is already True by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bERYP_HiqOkV",
        "outputId": "8ff6e248-cdb5-4829-82bd-70ca6dff1d8b"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\r\n",
        "\r\n",
        "sentences = [\"This document is the first document\",\r\n",
        "             \"This document is the second document\",\r\n",
        "             \"and this is the third one\"]\r\n",
        "\r\n",
        "xf = tfidf_vectorizer.fit_transform(sentences)\r\n",
        "\r\n",
        "xf.toarray()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.7284449 , 0.47890875, 0.28285122, 0.        ,\n",
              "        0.        , 0.28285122, 0.        , 0.28285122],\n",
              "       [0.        , 0.7284449 , 0.        , 0.28285122, 0.        ,\n",
              "        0.47890875, 0.28285122, 0.        , 0.28285122],\n",
              "       [0.49711994, 0.        , 0.        , 0.29360705, 0.49711994,\n",
              "        0.        , 0.29360705, 0.49711994, 0.29360705]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDsIlpwbqSJW",
        "outputId": "a20af0f4-8e69-4d7b-c395-106b1f74c4eb"
      },
      "source": [
        "new_sentence = [\"This is the new sentence\"]\r\n",
        "new_vectors = tfidf_vectorizer.transform(new_sentence)\r\n",
        "new_vectors.toarray()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.57735027, 0.        ,\n",
              "        0.        , 0.57735027, 0.        , 0.57735027]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgFQG0aqqZ31",
        "outputId": "8bf5337a-9726-42cb-db3d-fac4cbdaded6"
      },
      "source": [
        "tfidf_vectorizer.idf_"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.69314718, 1.28768207, 1.69314718, 1.        , 1.69314718,\n",
              "       1.69314718, 1.        , 1.69314718, 1.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}