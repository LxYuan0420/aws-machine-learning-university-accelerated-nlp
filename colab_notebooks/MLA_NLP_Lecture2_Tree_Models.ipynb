{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLA-NLP-Lecture2-Tree-Models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPlL4FPXg2g7s/1Lv4pXrGQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LxYuan0420/aws-machine-learning-university-accelerated-nlp/blob/master/colab_notebooks/MLA_NLP_Lecture2_Tree_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4GAH9TW-iud",
        "outputId": "6c9df516-d5bb-4311-aae4-dd91c3264783"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yGqpLQK-iue",
        "outputId": "7c36f7d8-df45-4a29-ef8f-88d11006800c"
      },
      "source": [
        "%cd /gdrive/MyDrive/Colab Notebooks/git/aws-machine-learning-university-accelerated-nlp/colab_notebooks"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/Colab Notebooks/git/aws-machine-learning-university-accelerated-nlp/colab_notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0igImjH9umt"
      },
      "source": [
        "**Machine Learning Accelerator - Natural Language Processing - Lecture 2**\r\n",
        "\r\n",
        "Tree-Based Models for a Classification Problem, and Hyperparameter Tuning\r\n",
        "We continue to work with our review dataset to see how Tree-based classifiers (Decision Tree, Random Forest), along with efficient optimization techniques (GridSearch, RandomizedSearch), perform to predict the isPositive field of our review dataset (that is very similar to the final project dataset)..\r\n",
        "\r\n",
        "1. Reading the dataset\r\n",
        "1. Exploratory data analysis\r\n",
        "1. Stop word removal and stemming\r\n",
        "1. Train - Validation Split\r\n",
        "1. Data processing with Pipeline and ColumnTransform\r\n",
        "1. Fit the Decision Tree classifier Find more details on the DecisionTreeClassifier here: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\r\n",
        "1. Test the classifier\r\n",
        "1. Fit and test the Random Forest classifier Find more details on the RandomForestClassifier here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\r\n",
        "1. Hyperparameter Tuning\r\n",
        "    1. Find more details on the GridSearchCV here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\r\n",
        "    1. Find more details on the RandomizedSearchCV here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\r\n",
        "1. Ideas for improvement\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Overall dataset schema:\r\n",
        "\r\n",
        "1. reviewText: Text of the review\r\n",
        "1. summary: Summary of the review\r\n",
        "1. verified: Whether the purchase was verified (True or False)\r\n",
        "1. time: UNIX timestamp for the review\r\n",
        "1. rating: Rating of the review\r\n",
        "1. log_votes: Logarithm-adjusted votes log(1+votes)\r\n",
        "1. isPositive: Whether the review is positive or negative (1 or 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Jk9fzP_-MQ8"
      },
      "source": [
        "**1. Reading the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "puoABFXF-W7D",
        "outputId": "012bd4f1-13fe-4ab7-dee3-f3addcc1efcb"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "df = pd.read_csv('../data/examples/AMAZON-REVIEW-DATA-CLASSIFICATION.csv')\r\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewText</th>\n",
              "      <th>summary</th>\n",
              "      <th>verified</th>\n",
              "      <th>time</th>\n",
              "      <th>log_votes</th>\n",
              "      <th>isPositive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PURCHASED FOR YOUNGSTER WHO\\nINHERITED MY \"TOO...</td>\n",
              "      <td>IDEAL FOR BEGINNER!</td>\n",
              "      <td>True</td>\n",
              "      <td>1361836800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>unable to open or use</td>\n",
              "      <td>Two Stars</td>\n",
              "      <td>True</td>\n",
              "      <td>1452643200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Waste of money!!! It wouldn't load to my system.</td>\n",
              "      <td>Dont buy it!</td>\n",
              "      <td>True</td>\n",
              "      <td>1433289600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I attempted to install this OS on two differen...</td>\n",
              "      <td>I attempted to install this OS on two differen...</td>\n",
              "      <td>True</td>\n",
              "      <td>1518912000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I've spent 14 fruitless hours over the past tw...</td>\n",
              "      <td>Do NOT Download.</td>\n",
              "      <td>True</td>\n",
              "      <td>1441929600</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          reviewText  ... isPositive\n",
              "0  PURCHASED FOR YOUNGSTER WHO\\nINHERITED MY \"TOO...  ...        1.0\n",
              "1                              unable to open or use  ...        0.0\n",
              "2   Waste of money!!! It wouldn't load to my system.  ...        0.0\n",
              "3  I attempted to install this OS on two differen...  ...        0.0\n",
              "4  I've spent 14 fruitless hours over the past tw...  ...        0.0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ2Xvb1O-d11"
      },
      "source": [
        "**2. Exploratory data analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPobsVcs-8fU",
        "outputId": "0e6678e8-00a0-44bc-bb9b-6ed408da2c41"
      },
      "source": [
        "df[\"isPositive\"].value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0    43692\n",
              "0.0    26308\n",
              "Name: isPositive, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--LXJzEj_HXF",
        "outputId": "877661aa-0092-4977-f8d5-501606f87d63"
      },
      "source": [
        "df.isna().sum()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "reviewText    11\n",
              "summary       14\n",
              "verified       0\n",
              "time           0\n",
              "log_votes      0\n",
              "isPositive     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSjjLGlY_Lnc"
      },
      "source": [
        "**3. Text Processing: Stop words removal and stemming**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z69dBL9_Uta",
        "outputId": "4ff93409-7a21-42e0-ae4c-12745111d692"
      },
      "source": [
        "import nltk\r\n",
        "\r\n",
        "nltk.download(\"punkt\")\r\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll7iPUdB_YQT"
      },
      "source": [
        "\r\n",
        "import nltk, re\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import SnowballStemmer\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "# Let's get a list of stop words from the NLTK library\r\n",
        "stop = stopwords.words('english')\r\n",
        "\r\n",
        "# These words are important for our problem. We don't want to remove them.\r\n",
        "excluding = ['against', 'not', 'don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\r\n",
        "             'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", \r\n",
        "             'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\",\r\n",
        "             'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \r\n",
        "             \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\r\n",
        "\r\n",
        "# New stop word list\r\n",
        "stop_words = [word for word in stop if word not in excluding]\r\n",
        "\r\n",
        "snow = SnowballStemmer('english')\r\n",
        "\r\n",
        "def process_text(texts): \r\n",
        "    final_text_list=[]\r\n",
        "    for sent in texts:\r\n",
        "        \r\n",
        "        # Check if the sentence is a missing value\r\n",
        "        if isinstance(sent, str) == False:\r\n",
        "            sent = \"\"\r\n",
        "            \r\n",
        "        filtered_sentence=[]\r\n",
        "        \r\n",
        "        sent = sent.lower() # Lowercase \r\n",
        "        sent = sent.strip() # Remove leading/trailing whitespace\r\n",
        "        sent = re.sub('\\s+', ' ', sent) # Remove extra space and tabs\r\n",
        "        sent = re.compile('<.*?>').sub('', sent) # Remove HTML tags/markups:\r\n",
        "        \r\n",
        "        for w in word_tokenize(sent):\r\n",
        "            # We are applying some custom filtering here, feel free to try different things\r\n",
        "            # Check if it is not numeric and its length>2 and not in stop words\r\n",
        "            if(not w.isnumeric()) and (len(w)>2) and (w not in stop_words):  \r\n",
        "                # Stem and add to filtered list\r\n",
        "                filtered_sentence.append(snow.stem(w))\r\n",
        "        final_string = \" \".join(filtered_sentence) #final string of cleaned words\r\n",
        " \r\n",
        "        final_text_list.append(final_string)\r\n",
        "        \r\n",
        "    return final_text_list"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka8sUwd6_gPr"
      },
      "source": [
        "**4. Train - validation Split**\r\n",
        "\r\n",
        "9/1 split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkSmRYU5_lDE"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(df[[\"reviewText\", \"summary\", \"time\", \"log_votes\"]],\r\n",
        "                                                  df[\"isPositive\"],\r\n",
        "                                                  test_size=0.1,\r\n",
        "                                                  shuffle=True,\r\n",
        "                                                  random_state=42)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR2bjA8PBavo",
        "outputId": "fed66e52-f1d6-412b-df34-d0da52907d74"
      },
      "source": [
        "print(\"Processing the reviewText field\")\r\n",
        "\r\n",
        "X_train[\"reviewText\"] = process_text(X_train[\"reviewText\"].tolist())\r\n",
        "X_val[\"reviewText\"] = process_text(X_val[\"reviewText\"].tolist())\r\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing the reviewText field\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi2YnfIEBuRW"
      },
      "source": [
        "X_train[\"summary\"] = process_text(X_train[\"summary\"].tolist())\r\n",
        "X_val[\"summary\"] = process_text(X_val[\"summary\"].tolist())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUSBfmapB1Fd"
      },
      "source": [
        "**5. Data processing with Pipeline and ColumnTransform**\r\n",
        "\r\n",
        "In the previous examples, we have seen how to use pipeline to prepare a data field for our machine learning model. This time, we will focus on multiple fields: numeric and text fields. We are using linear regression model from Sklearn: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model.\r\n",
        "\r\n",
        "For the numerical features pipeline, the numerical_processor below, we use a MinMaxScaler (don't have to scale features when using Decision Trees, but it's a good idea to see how to use more data transforms). If different processing is desired for different numerical features, different pipelines should be built - just like shown below for the two text features.\r\n",
        "For the numerical features pipeline, the text_processor below, we use CountVectorizer() for the text fields.\r\n",
        "The selective preparations of the dataset features are then put together into a collective ColumnTransformer, to be finally used in a Pipeline along with an estimator. This ensures that the transforms are performed automatically on the raw data when fitting the model and when making predictions, such as when evaluating the model on a validation dataset via cross-validation or making predictions on a test dataset in the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qwMwMWzCbBq"
      },
      "source": [
        "numerical_features = [\"time\", \"log_votes\"]\r\n",
        "text_features = [\"summary\", \"reviewText\"]\r\n",
        "\r\n",
        "model_features = numerical_features + text_features\r\n",
        "model_target = \"isPositive\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZGPPQfYB57s"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.compose import ColumnTransformer\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "\r\n",
        "# Column Transformer\r\n",
        "numerical_processor = Pipeline([(\"num_scalar\", MinMaxScaler())])\r\n",
        "\r\n",
        "# preprocess text features\r\n",
        "text_preprocessor_0 = Pipeline([(\"text_vect_0\", CountVectorizer(binary=True, max_features=50))])\r\n",
        "text_preprocessor_1 = Pipeline([(\"text_vect_1\", CountVectorizer(binary=True, max_features=150))])\r\n",
        "\r\n",
        "data_preprocessor = ColumnTransformer([\r\n",
        "    (\"numerical_pre\", numerical_processor, numerical_features),\r\n",
        "    (\"text_pre_0\", text_preprocessor_0, text_features[0]),\r\n",
        "    (\"text_pre_1\", text_preprocessor_1, text_features[1])\r\n",
        "])\r\n",
        "\r\n",
        "\r\n",
        "# Pipeline \r\n",
        "pipeline = Pipeline([\r\n",
        "    (\"data_preprocessing\", data_preprocessor),\r\n",
        "    (\"DecisionTreeClassifiern\", DecisionTreeClassifier(max_depth=10, min_samples_leaf=15))\r\n",
        "])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUNlyzsaFIm-"
      },
      "source": [
        "**6. Train Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSAdRsEtFZoE",
        "outputId": "5537dd78-cf91-4565-8e6f-af54e4c281ca"
      },
      "source": [
        "pipeline.fit(X_train, y_train.values)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('data_preprocessing',\n",
              "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
              "                                   sparse_threshold=0.3,\n",
              "                                   transformer_weights=None,\n",
              "                                   transformers=[('numerical_pre',\n",
              "                                                  Pipeline(memory=None,\n",
              "                                                           steps=[('num_scalar',\n",
              "                                                                   MinMaxScaler(copy=True,\n",
              "                                                                                feature_range=(0,\n",
              "                                                                                               1)))],\n",
              "                                                           verbose=False),\n",
              "                                                  ['time', 'log_votes']),\n",
              "                                                 ('text_pre_0',\n",
              "                                                  Pipeline(memory=None,\n",
              "                                                           steps=[('text_ve...\n",
              "                                   verbose=False)),\n",
              "                ('DecisionTreeClassifiern',\n",
              "                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
              "                                        criterion='gini', max_depth=10,\n",
              "                                        max_features=None, max_leaf_nodes=None,\n",
              "                                        min_impurity_decrease=0.0,\n",
              "                                        min_impurity_split=None,\n",
              "                                        min_samples_leaf=15,\n",
              "                                        min_samples_split=2,\n",
              "                                        min_weight_fraction_leaf=0.0,\n",
              "                                        presort='deprecated', random_state=None,\n",
              "                                        splitter='best'))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQEyulqHFfBl"
      },
      "source": [
        "**7. Fitting Linear Regression models and checking the validation performance**\r\n",
        "\r\n",
        "7.1 LinearRegression\r\n",
        "Let's first fit LinearRegression from Sklearn library, and check the performance on the validation dataset. Using the coef_ atribute, we can also print the learned weights of the model.\r\n",
        "\r\n",
        "Find more details on LinearRegression here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJqlKH3cFjyM",
        "outputId": "85821e82-202c-4b52-98d3-07a64c2fd941"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\r\n",
        "\r\n",
        "val_pred = pipeline.predict(X_val)\r\n",
        "print(classification_report(y_val.values, val_pred))\r\n",
        "print(f\"Accuracy on validation set: {accuracy_score(y_val.values, val_pred)}\")\r\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.71      0.71      2557\n",
            "         1.0       0.83      0.83      0.83      4443\n",
            "\n",
            "    accuracy                           0.79      7000\n",
            "   macro avg       0.77      0.77      0.77      7000\n",
            "weighted avg       0.79      0.79      0.79      7000\n",
            "\n",
            "Accuracy on validation set: 0.7872857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUbpqZkpFtj1"
      },
      "source": [
        "**8. Fit and test the Random Forest classifier¶**\r\n",
        "(Go to top)\r\n",
        "\r\n",
        "This time, we will use the Random Forest classifier. Let's update our pipeline for that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xlycbAqFyPs",
        "outputId": "c8850bc4-77be-4504-f512-4d6c56d4ce0d"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "\r\n",
        "pipeline = Pipeline([\r\n",
        "    (\"data_preprocessing\", data_preprocessor),\r\n",
        "    (\"rf\", RandomForestClassifier(n_estimators=150, max_depth=10, min_samples_leaf=15))\r\n",
        "])\r\n",
        "\r\n",
        "pipeline.fit(X_train[model_features], y_train.values)\r\n",
        "\r\n",
        "val_predictions = pipeline.predict(X_val)\r\n",
        "print(confusion_matrix(y_val.values, val_predictions))\r\n",
        "print(classification_report(y_val.values, val_predictions))\r\n",
        "print(\"Accuracy (validation):\", accuracy_score(y_val.values, val_predictions))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/compose/_column_transformer.py:430: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1634  923]\n",
            " [ 403 4040]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.64      0.71      2557\n",
            "         1.0       0.81      0.91      0.86      4443\n",
            "\n",
            "    accuracy                           0.81      7000\n",
            "   macro avg       0.81      0.77      0.79      7000\n",
            "weighted avg       0.81      0.81      0.81      7000\n",
            "\n",
            "Accuracy (validation): 0.8105714285714286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kuyh7mcGNLP"
      },
      "source": [
        "**9. Hyperparameter Tuning**\r\n",
        "\r\n",
        "Let's try different parameter values and see how the DecisionTreeClassifier model performs under some combinations of parameters.\r\n",
        "\r\n",
        "Warning: The number of hyperparameters tuned, along with the cross-validations, can greatly increase training time! Especially if trying hyperparameters tuning on the RandomForestClassifier instead of the lower performing DecisionTreeClassifier that we showcase below for speed! Similar tuning on a RandomForestClassifier model can take more minutes to hours!\r\n",
        "\r\n",
        "9.1 GridSearchCV\r\n",
        "Find more details on the GridSearchCV here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPxiedpsGRT0",
        "outputId": "17557fef-6c62-4025-f0d3-45016ee3f831"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\r\n",
        "\r\n",
        "#parameter grid\r\n",
        "param_grid = {\r\n",
        "    \"rf__max_depth\": [10,20],\r\n",
        "    \"rf__min_samples_leaf\": [5,10]\r\n",
        "}\r\n",
        "\r\n",
        "grid_search = GridSearchCV(pipeline,\r\n",
        "                           param_grid,\r\n",
        "                           cv=3,\r\n",
        "                           verbose=10,\r\n",
        "                           n_jobs=-1)\r\n",
        "\r\n",
        "grid_search.fit(X_train, y_train)\r\n",
        "\r\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\r\n",
        "print(f\"Best score: {grid_search.best_score_}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   18.3s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   34.6s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:  3.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters: {'rf__max_depth': 20, 'rf__min_samples_leaf': 5}\n",
            "Best score: 0.8341269841269842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vPZ8g0WZcJ5",
        "outputId": "4c4696d1-0d70-447b-df23-9d333d134aeb"
      },
      "source": [
        "val_predictions = grid_search.best_estimator_.predict(X_val)\r\n",
        "print(confusion_matrix(y_val.values, val_predictions))\r\n",
        "print(classification_report(y_val.values, val_predictions))\r\n",
        "print(\"Accuracy (validation):\", accuracy_score(y_val.values, val_predictions))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1836  721]\n",
            " [ 469 3974]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.72      0.76      2557\n",
            "         1.0       0.85      0.89      0.87      4443\n",
            "\n",
            "    accuracy                           0.83      7000\n",
            "   macro avg       0.82      0.81      0.81      7000\n",
            "weighted avg       0.83      0.83      0.83      7000\n",
            "\n",
            "Accuracy (validation): 0.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEV66kpAfepX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}