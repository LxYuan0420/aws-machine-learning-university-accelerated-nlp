{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLA-NLP-Lecture1-Text-Process.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4U+5vzWqYPN3ESfxLrgts",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LxYuan0420/aws-machine-learning-university-accelerated-nlp/blob/master/colab_notebooks/MLA_NLP_Lecture1_Text_Process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRv0ppJ1dyKe"
      },
      "source": [
        "**Text Processing**\r\n",
        "\r\n",
        "In this notebook, we go over some simple techniques to clean and prepare text data for modeling with machine learning.\r\n",
        "\r\n",
        "1. Simple text cleaning processes\r\n",
        "2. Lexicon-based text processing\r\n",
        "    - Stop words removal\r\n",
        "    - Stemming\r\n",
        "    - Lemmatization\r\n",
        "\r\n",
        "    \r\n",
        "**1. Simple text cleaning processes**\r\n",
        "\r\n",
        "\r\n",
        "In this section, we will do some general purpose text cleaning. The following methods for cleaning can be extended depending on the application."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGhjq-lmeCSD",
        "outputId": "d6e98842-0a5d-41da-dc4a-49b810ec8669"
      },
      "source": [
        "text = \"   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .  \"\r\n",
        "print(text)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE8q4rv9eFUp",
        "outputId": "375058da-3e44-487b-f51e-07b858f2e4fd"
      },
      "source": [
        "# Lowercase\r\n",
        "print(text.lower())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   this is a message to be cleaned. it may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQrOQISQeI-o",
        "outputId": "ff7e520e-2a1a-42f5-bf0f-2254f0eed883"
      },
      "source": [
        "# remove leading/trailing whitespace \r\n",
        "print(text.strip())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mljUNSZneNKH",
        "outputId": "0d671ea9-b77c-4fab-98a7-86c6bdf708b7"
      },
      "source": [
        "# remove html tags/markups\r\n",
        "import re\r\n",
        "\r\n",
        "text = re.compile(\"<.*?>\").sub(\"\", text)\r\n",
        "print(text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   This is a message to be cleaned. It may involve some things like: , ?, :, ''  adjacent spaces and tabs     .  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpCVYB13efqR",
        "outputId": "54e5458f-ba46-4df3-b010-7be3b1f8b5b4"
      },
      "source": [
        "# replace punctuation with space.\r\n",
        "# punct can actually be useful sometimes.\r\n",
        "import re, string\r\n",
        "\r\n",
        "text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\r\n",
        "print(text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   This is a message to be cleaned  It may involve some things like              adjacent spaces and tabs        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSYxgneIe7sA",
        "outputId": "f72eccc4-e6eb-41c3-a376-9d7e6be35d61"
      },
      "source": [
        "print(re.escape(string.punctuation))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^_\\`\\{\\|\\}\\~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj9obC5XfAkI",
        "outputId": "cde825e0-f4d5-4518-e410-eb9697ee5000"
      },
      "source": [
        "print(string.punctuation)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7vUmAF4fDDH"
      },
      "source": [
        "**2. Lexicon-based text processing**\r\n",
        "\r\n",
        "We saw some general purpose text pre-processing methods in the previous section. Lexicon based methods are usually applied after the common text processing methods. They are used to normalize sentences in our dataset. By normalization, here, we mean putting words into a similar format that will also enhace similarities (if any) between sentences.\r\n",
        "\r\n",
        "We need to install some packages for this example. Run the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fCkk9yHfJ7H",
        "outputId": "34e794f7-1903-4305-f049-331e67e855a5"
      },
      "source": [
        "import nltk\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdMdVdlLfSh4"
      },
      "source": [
        "# remove stop words\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "filtered_sentence = []\r\n",
        "\r\n",
        "stop_words = [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\"]\r\n",
        "\r\n",
        "# tokenize each sentence\r\n",
        "words = word_tokenize(text)\r\n",
        "for w in words:\r\n",
        "    if w not in stop_words:\r\n",
        "        filtered_sentence.append(w)\r\n",
        "    \r\n",
        "text = \" \".join(filtered_sentence)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihBvEPk6fpza",
        "outputId": "504ab92b-9711-42a5-dc12-7a8a56b4286d"
      },
      "source": [
        "print(text)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This message be cleaned It may involve some things like adjacent spaces tabs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNVlpqcMfvkx"
      },
      "source": [
        "**Stemming**\r\n",
        "\r\n",
        "Stemming is a rule-based system to convert words into their root form. It removes suffixes from words. This helps us enhace similarities (if any) between sentences.\r\n",
        "\r\n",
        "Example:\r\n",
        "\r\n",
        "\"jumping\", \"jumped\" -> \"jump\"\r\n",
        "\r\n",
        "\"cars\" -> \"car\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUAS2uhbgIlX",
        "outputId": "e94634c8-9fc7-402e-a6d2-338b7ea20d70"
      },
      "source": [
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.stem import SnowballStemmer\r\n",
        "\r\n",
        "text = \"   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .  \"\r\n",
        "\r\n",
        "tokenized_words = word_tokenize(text)\r\n",
        "\r\n",
        "snow = SnowballStemmer('english')\r\n",
        "stem_words = [snow.stem(w) for w in tokenized_words]\r\n",
        "\r\n",
        "for w, s in zip(tokenized_words, stem_words):\r\n",
        "    print(f\"{w:<10} -> {s}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This       -> this\n",
            "is         -> is\n",
            "a          -> a\n",
            "message    -> messag\n",
            "to         -> to\n",
            "be         -> be\n",
            "cleaned    -> clean\n",
            ".          -> .\n",
            "It         -> it\n",
            "may        -> may\n",
            "involve    -> involv\n",
            "some       -> some\n",
            "things     -> thing\n",
            "like       -> like\n",
            ":          -> :\n",
            "<          -> <\n",
            "br         -> br\n",
            ">          -> >\n",
            ",          -> ,\n",
            "?          -> ?\n",
            ",          -> ,\n",
            ":          -> :\n",
            ",          -> ,\n",
            "''         -> ''\n",
            "adjacent   -> adjac\n",
            "spaces     -> space\n",
            "and        -> and\n",
            "tabs       -> tab\n",
            ".          -> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u65W71Q1ghfq"
      },
      "source": [
        "You can see above that stemming operation is NOT perfect. We have mistakes such as \"messag\", \"involv\", \"adjac\". It is a rule based method that sometimes mistakely remove suffixes from words. Nevertheless, it runs fast."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXLrsIZThZmn"
      },
      "source": [
        "**Lemmatization**\r\n",
        "\r\n",
        "If we are not satisfied with the result of stemming, we can use the Lemmatization instead. It usually requires more work, but gives better results. As mentioned in the class, lemmatization needs to know the correct word position tags such as \"noun\", \"verb\", \"adjective\", etc. and we will use another NLTK function to feed this information to the lemmatizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gomMSTTdly5-",
        "outputId": "5529dc72-bfb3-487f-8ccf-05262fc12886"
      },
      "source": [
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.corpus import wordnet\r\n",
        "\r\n",
        "# initialie the lemmatizer\r\n",
        "wl = WordNetLemmatizer()\r\n",
        "\r\n",
        "# This is a helper function to map NLTK position tags\r\n",
        "# Full list is available here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\r\n",
        "def get_wordnet_pos(tag):\r\n",
        "    if tag.startswith(\"J\"):\r\n",
        "        return wordnet.ADJ\r\n",
        "    elif tag.startswith(\"V\"):\r\n",
        "        return wordnet.VERB\r\n",
        "    elif tag.startswith(\"N\"):\r\n",
        "        return wordnet.NOUN\r\n",
        "    else:\r\n",
        "        return wordnet.ADV\r\n",
        "\r\n",
        "text = \"   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .  \"\r\n",
        "\r\n",
        "lemmatized_sentence = []\r\n",
        "words = word_tokenize(text)\r\n",
        "word_pos_tags = nltk.pos_tag(words)\r\n",
        "for idx, tag in enumerate(word_pos_tags):\r\n",
        "    lemmatized_sentence.append(wl.lemmatize(tag[0], get_wordnet_pos(tag[1])))\r\n",
        "\r\n",
        "lemmatized_text = \" \".join(lemmatized_sentence)\r\n",
        "print(lemmatized_text)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This be a message to be clean . It may involve some thing like : < br > , ? , : , '' adjacent space and tab .\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}