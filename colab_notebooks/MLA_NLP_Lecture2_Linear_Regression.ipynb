{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLA-NLP-Lecture2-Linear-Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOXLkN89k8VGtJuHY4rNtU9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LxYuan0420/aws-machine-learning-university-accelerated-nlp/blob/master/colab_notebooks/MLA_NLP_Lecture2_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4GAH9TW-iud",
        "outputId": "6587e7d5-336c-4d8e-9e89-42925d79988b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yGqpLQK-iue",
        "outputId": "22123e72-dd13-4a48-f5d4-203b6a6773a5"
      },
      "source": [
        "%cd /gdrive/MyDrive/Colab Notebooks/git/aws-machine-learning-university-accelerated-nlp/colab_notebooks"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/Colab Notebooks/git/aws-machine-learning-university-accelerated-nlp/colab_notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0igImjH9umt"
      },
      "source": [
        "**Machine Learning Accelerator - Natural Language Processing - Lecture 2**\r\n",
        "\r\n",
        "Linear Regression Models and Regularization\r\n",
        "In this notebook, we go over Linear Regression methods (with and without regularization: LinearRegression, Ridge, Lasso, ElasticNet) to predict the log_votes field of our review dataset.\r\n",
        "\r\n",
        "1. Reading the dataset\r\n",
        "1. Exploratory data analysis\r\n",
        "1. Stop word removal and stemming\r\n",
        "1. Train - Validation Split\r\n",
        "1. Data processing with Pipeline and ColumnTransform\r\n",
        "1. Train the regressor\r\n",
        "1. Fitting Linear Regression models and checking the validation performance Find more details on the classical Linear Regression models with and without regularization here: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model\r\n",
        "1. Ideas for improvement\r\n",
        "\r\n",
        "\r\n",
        "Overall dataset schema:\r\n",
        "\r\n",
        "1. reviewText: Text of the review\r\n",
        "1. summary: Summary of the review\r\n",
        "1. verified: Whether the purchase was verified (True or False)\r\n",
        "1. time: UNIX timestamp for the review\r\n",
        "1. rating: Rating of the review\r\n",
        "1. log_votes: Logarithm-adjusted votes log(1+votes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Jk9fzP_-MQ8"
      },
      "source": [
        "**1. Reading the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "puoABFXF-W7D",
        "outputId": "577510b7-ed72-4822-c587-6ed5a706d526"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "df = pd.read_csv('../data/examples/AMAZON-REVIEW-DATA-REGRESSION.csv')\r\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewText</th>\n",
              "      <th>summary</th>\n",
              "      <th>verified</th>\n",
              "      <th>time</th>\n",
              "      <th>rating</th>\n",
              "      <th>log_votes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Stuck with this at work, slow and we still got...</td>\n",
              "      <td>Use SEP or Mcafee</td>\n",
              "      <td>False</td>\n",
              "      <td>1464739200</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I use parallels every day with both my persona...</td>\n",
              "      <td>Use it daily</td>\n",
              "      <td>False</td>\n",
              "      <td>1332892800</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Barbara Robbins\\n\\nI've used TurboTax to do ou...</td>\n",
              "      <td>Helpful Product</td>\n",
              "      <td>True</td>\n",
              "      <td>1398816000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I have been using this software security for y...</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>True</td>\n",
              "      <td>1430784000</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>If you want your computer hijacked and slowed ...</td>\n",
              "      <td>... hijacked and slowed to a crawl Windows 10 ...</td>\n",
              "      <td>False</td>\n",
              "      <td>1508025600</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          reviewText  ... log_votes\n",
              "0  Stuck with this at work, slow and we still got...  ...       0.0\n",
              "1  I use parallels every day with both my persona...  ...       0.0\n",
              "2  Barbara Robbins\\n\\nI've used TurboTax to do ou...  ...       0.0\n",
              "3  I have been using this software security for y...  ...       0.0\n",
              "4  If you want your computer hijacked and slowed ...  ...       0.0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ2Xvb1O-d11"
      },
      "source": [
        "**2. Exploratory data analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPobsVcs-8fU",
        "outputId": "c0b50f6d-106f-4d45-d53e-6b854d103d7e"
      },
      "source": [
        "df[\"log_votes\"].min()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBDwXEO3--Tl",
        "outputId": "e1511c85-a149-4a81-dd37-0a3ebed55edd"
      },
      "source": [
        "df[\"log_votes\"].max()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.799753318287247"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "o0gQbMKw_A8s",
        "outputId": "e46056cc-553f-4036-e8db-f6969cb44a5c"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "df['log_votes'].plot.hist()\r\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--LXJzEj_HXF",
        "outputId": "1f4b480a-da59-48e8-f033-e6ecc281cfc6"
      },
      "source": [
        "df.isna().sum()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "reviewText    6\n",
              "summary       7\n",
              "verified      0\n",
              "time          0\n",
              "rating        0\n",
              "log_votes     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSjjLGlY_Lnc"
      },
      "source": [
        "**3. Text Processing: Stop words removal and stemming**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z69dBL9_Uta",
        "outputId": "0cd7162e-13bb-45cc-8565-666892cc9b2f"
      },
      "source": [
        "import nltk\r\n",
        "\r\n",
        "nltk.download(\"punkt\")\r\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll7iPUdB_YQT"
      },
      "source": [
        "\r\n",
        "import nltk, re\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import SnowballStemmer\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "# Let's get a list of stop words from the NLTK library\r\n",
        "stop = stopwords.words('english')\r\n",
        "\r\n",
        "# These words are important for our problem. We don't want to remove them.\r\n",
        "excluding = ['against', 'not', 'don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\r\n",
        "             'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", \r\n",
        "             'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\",\r\n",
        "             'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \r\n",
        "             \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\r\n",
        "\r\n",
        "# New stop word list\r\n",
        "stop_words = [word for word in stop if word not in excluding]\r\n",
        "\r\n",
        "snow = SnowballStemmer('english')\r\n",
        "\r\n",
        "def process_text(texts): \r\n",
        "    final_text_list=[]\r\n",
        "    for sent in texts:\r\n",
        "        \r\n",
        "        # Check if the sentence is a missing value\r\n",
        "        if isinstance(sent, str) == False:\r\n",
        "            sent = \"\"\r\n",
        "            \r\n",
        "        filtered_sentence=[]\r\n",
        "        \r\n",
        "        sent = sent.lower() # Lowercase \r\n",
        "        sent = sent.strip() # Remove leading/trailing whitespace\r\n",
        "        sent = re.sub('\\s+', ' ', sent) # Remove extra space and tabs\r\n",
        "        sent = re.compile('<.*?>').sub('', sent) # Remove HTML tags/markups:\r\n",
        "        \r\n",
        "        for w in word_tokenize(sent):\r\n",
        "            # We are applying some custom filtering here, feel free to try different things\r\n",
        "            # Check if it is not numeric and its length>2 and not in stop words\r\n",
        "            if(not w.isnumeric()) and (len(w)>2) and (w not in stop_words):  \r\n",
        "                # Stem and add to filtered list\r\n",
        "                filtered_sentence.append(snow.stem(w))\r\n",
        "        final_string = \" \".join(filtered_sentence) #final string of cleaned words\r\n",
        " \r\n",
        "        final_text_list.append(final_string)\r\n",
        "        \r\n",
        "    return final_text_list"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka8sUwd6_gPr"
      },
      "source": [
        "**4. Train - validation Split**\r\n",
        "\r\n",
        "9/1 split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkSmRYU5_lDE"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(df[[\"reviewText\", \"summary\", \"time\", \"rating\"]],\r\n",
        "                                                  df[\"log_votes\"],\r\n",
        "                                                  test_size=0.1,\r\n",
        "                                                  shuffle=True,\r\n",
        "                                                  random_state=42)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR2bjA8PBavo",
        "outputId": "adc54beb-6a71-4c47-acc1-78d50baac2ce"
      },
      "source": [
        "print(\"Processing the reviewText field\")\r\n",
        "\r\n",
        "X_train[\"reviewText\"] = process_text(X_train[\"reviewText\"].tolist())\r\n",
        "X_val[\"reviewText\"] = process_text(X_val[\"reviewText\"].tolist())\r\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing the reviewText field\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi2YnfIEBuRW"
      },
      "source": [
        "X_train[\"summary\"] = process_text(X_train[\"summary\"].tolist())\r\n",
        "X_val[\"summary\"] = process_text(X_val[\"summary\"].tolist())"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUSBfmapB1Fd"
      },
      "source": [
        "**5. Data processing with Pipeline and ColumnTransform**\r\n",
        "\r\n",
        "In the previous examples, we have seen how to use pipeline to prepare a data field for our machine learning model. This time, we will focus on multiple fields: numeric and text fields. We are using linear regression model from Sklearn: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model.\r\n",
        "\r\n",
        "For the numerical features pipeline, the numerical_processor below, we use a MinMaxScaler (don't have to scale features when using Decision Trees, but it's a good idea to see how to use more data transforms). If different processing is desired for different numerical features, different pipelines should be built - just like shown below for the two text features.\r\n",
        "For the numerical features pipeline, the text_processor below, we use CountVectorizer() for the text fields.\r\n",
        "The selective preparations of the dataset features are then put together into a collective ColumnTransformer, to be finally used in a Pipeline along with an estimator. This ensures that the transforms are performed automatically on the raw data when fitting the model and when making predictions, such as when evaluating the model on a validation dataset via cross-validation or making predictions on a test dataset in the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qwMwMWzCbBq"
      },
      "source": [
        "numerical_features = [\"time\", \"rating\"]\r\n",
        "text_features = [\"summary\", \"reviewText\"]\r\n",
        "\r\n",
        "model_features = numerical_features + text_features\r\n",
        "model_target = \"log_votes\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZGPPQfYB57s"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.compose import ColumnTransformer\r\n",
        "from sklearn.linear_model import LinearRegression\r\n",
        "\r\n",
        "# Column Transformer\r\n",
        "numerical_processor = Pipeline([(\"num_scalar\", MinMaxScaler())])\r\n",
        "\r\n",
        "# preprocess text features\r\n",
        "text_preprocessor_0 = Pipeline([(\"text_vect_0\", CountVectorizer(binary=True, max_features=50))])\r\n",
        "text_preprocessor_1 = Pipeline([(\"text_vect_1\", CountVectorizer(binary=True, max_features=150))])\r\n",
        "\r\n",
        "data_preprocessor = ColumnTransformer([\r\n",
        "    (\"numerical_pre\", numerical_processor, numerical_features),\r\n",
        "    (\"text_pre_0\", text_preprocessor_0, text_features[0]),\r\n",
        "    (\"text_pre_1\", text_preprocessor_1, text_features[1])\r\n",
        "])\r\n",
        "\r\n",
        "\r\n",
        "# Pipeline \r\n",
        "pipeline = Pipeline([\r\n",
        "    (\"data_preprocessing\", data_preprocessor),\r\n",
        "    (\"lr\", LinearRegression())\r\n",
        "])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhuMwgvTEjNR",
        "outputId": "529cd7b3-c9e2-4fea-d695-1a24a3a88580"
      },
      "source": [
        "pipeline"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('data_preprocessing',\n",
              "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
              "                                   sparse_threshold=0.3,\n",
              "                                   transformer_weights=None,\n",
              "                                   transformers=[('numerical_pre',\n",
              "                                                  Pipeline(memory=None,\n",
              "                                                           steps=[('num_scalar',\n",
              "                                                                   MinMaxScaler(copy=True,\n",
              "                                                                                feature_range=(0,\n",
              "                                                                                               1)))],\n",
              "                                                           verbose=False),\n",
              "                                                  ['time', 'rating']),\n",
              "                                                 ('text_pre_0',\n",
              "                                                  Pipeline(memory=None,\n",
              "                                                           steps=[('text_vect_...\n",
              "                                                                                   input='content',\n",
              "                                                                                   lowercase=True,\n",
              "                                                                                   max_df=1.0,\n",
              "                                                                                   max_features=150,\n",
              "                                                                                   min_df=1,\n",
              "                                                                                   ngram_range=(1,\n",
              "                                                                                                1),\n",
              "                                                                                   preprocessor=None,\n",
              "                                                                                   stop_words=None,\n",
              "                                                                                   strip_accents=None,\n",
              "                                                                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                                                                   tokenizer=None,\n",
              "                                                                                   vocabulary=None))],\n",
              "                                                           verbose=False),\n",
              "                                                  'reviewText')],\n",
              "                                   verbose=False)),\n",
              "                ('lr',\n",
              "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
              "                                  normalize=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUNlyzsaFIm-"
      },
      "source": [
        "**6. Train Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSAdRsEtFZoE",
        "outputId": "ca27c951-d5f4-4423-df5c-6fc0ee05cac7"
      },
      "source": [
        "pipeline.fit(X_train[model_features], y_train.values)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('data_preprocessing',\n",
              "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
              "                                   sparse_threshold=0.3,\n",
              "                                   transformer_weights=None,\n",
              "                                   transformers=[('numerical_pre',\n",
              "                                                  Pipeline(memory=None,\n",
              "                                                           steps=[('num_scalar',\n",
              "                                                                   MinMaxScaler(copy=True,\n",
              "                                                                                feature_range=(0,\n",
              "                                                                                               1)))],\n",
              "                                                           verbose=False),\n",
              "                                                  ['time', 'rating']),\n",
              "                                                 ('text_pre_0',\n",
              "                                                  Pipeline(memory=None,\n",
              "                                                           steps=[('text_vect_...\n",
              "                                                                                   input='content',\n",
              "                                                                                   lowercase=True,\n",
              "                                                                                   max_df=1.0,\n",
              "                                                                                   max_features=150,\n",
              "                                                                                   min_df=1,\n",
              "                                                                                   ngram_range=(1,\n",
              "                                                                                                1),\n",
              "                                                                                   preprocessor=None,\n",
              "                                                                                   stop_words=None,\n",
              "                                                                                   strip_accents=None,\n",
              "                                                                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                                                                   tokenizer=None,\n",
              "                                                                                   vocabulary=None))],\n",
              "                                                           verbose=False),\n",
              "                                                  'reviewText')],\n",
              "                                   verbose=False)),\n",
              "                ('lr',\n",
              "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
              "                                  normalize=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQEyulqHFfBl"
      },
      "source": [
        "**7. Fitting Linear Regression models and checking the validation performance**\r\n",
        "\r\n",
        "7.1 LinearRegression\r\n",
        "Let's first fit LinearRegression from Sklearn library, and check the performance on the validation dataset. Using the coef_ atribute, we can also print the learned weights of the model.\r\n",
        "\r\n",
        "Find more details on LinearRegression here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJqlKH3cFjyM",
        "outputId": "eb1427df-3a19-47d7-9a50-058f6ca11931"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\r\n",
        "from sklearn.metrics import r2_score, mean_squared_error\r\n",
        "\r\n",
        "lrRegressor_val_predictions = pipeline.predict(X_val[model_features])\r\n",
        "print(\"LinearRegression on Validation: Mean_squared_error: %f,  R_square_score: %f\" % \\\r\n",
        "      (mean_squared_error(y_val, lrRegressor_val_predictions),r2_score(y_val, lrRegressor_val_predictions)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearRegression on Validation: Mean_squared_error: 0.648439,  R_square_score: 0.362022\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUbpqZkpFtj1"
      },
      "source": [
        "**7.2 Ridge (Linear Regression with L2 regularization)**\r\n",
        "\r\n",
        "Let's now fit Ridge from Sklearn library, and check the performance on the validation dataset.\r\n",
        "\r\n",
        "Find more details on Ridge here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\r\n",
        "\r\n",
        "To improve the performance of a LinearRegression model, Ridge is tuning model complexity by adding a $L_2$ penalty score for complexity to the model cost function:\r\n",
        "\r\n",
        "$$\\text{C}_{\\text{regularized}}(\\textbf{w}) = \\text{C}(\\textbf{w}) +  {alpha}∗||\\textbf{w}||_2^2$$\r\n",
        "where $\\textbf{w}$ is the model weights vector, and $||\\textbf{w}||_2^2 = \\sum \\textbf{w}_i^2$.\r\n",
        "\r\n",
        "The strength of the regularization is controlled by the regularizer parameter, alpha: smaller value of $alpha$, weaker regularization; larger value of $alpha$, stronger regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xlycbAqFyPs",
        "outputId": "3943ad56-e41a-4ad3-c9b0-0823bc033cec"
      },
      "source": [
        "from sklearn.linear_model import Ridge\r\n",
        "from sklearn.metrics import r2_score, mean_squared_error\r\n",
        "\r\n",
        "ridge_pipeline = Pipeline([\r\n",
        "    (\"data_preprocessing\", data_preprocessor),\r\n",
        "    (\"ridge\", Ridge(alpha=100))\r\n",
        "])\r\n",
        "\r\n",
        "ridge_pipeline.fit(X_train[model_features], y_train.values)\r\n",
        "ridgeRegressor_val_predictions = ridge_pipeline.predict(X_val[model_features])\r\n",
        "\r\n",
        "print(\"Ridge on Validation: Mean_squared_error: %f,  R_square_score: %f\" % \\\r\n",
        "      (mean_squared_error(y_val, ridgeRegressor_val_predictions),r2_score(y_val, ridgeRegressor_val_predictions)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ridge on Validation: Mean_squared_error: 0.649729,  R_square_score: 0.360753\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kuyh7mcGNLP"
      },
      "source": [
        "**7.3 LASSO (Linear Regression with L1 regularization)**\r\n",
        "\r\n",
        "Let's also fit Lasso from Sklearn library, and check the performance on the validation dataset.\r\n",
        "\r\n",
        "Find more details on Lasso here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\r\n",
        "\r\n",
        "Lasso is tuning model complexity by adding a $L_1$ penalty score for complexity to the model cost function:\r\n",
        "\r\n",
        "$$\\text{C}_{\\text{regularized}}(\\textbf{w}) = \\text{C}(\\textbf{w}) +  alpha∗||\\textbf{w}||_1$$\r\n",
        "where $\\textbf{w}$ is the model weights vector, and $||\\textbf{w}||_1 = \\sum |\\textbf{w}_i|$.\r\n",
        "\r\n",
        "Again, the strength of the regularization is controlled by the regularizer parameter, $alpha$. Due to the geometry of $L_1$ norm, with Lasso, some of the weights will shrink all the way to 0, leading to sparsity - some of the features are not contributing to the model afterall!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPxiedpsGRT0",
        "outputId": "979b4d42-b4d9-495e-fa27-bdc3df8c4ea9"
      },
      "source": [
        "\r\n",
        "from sklearn.linear_model import Lasso\r\n",
        "from sklearn.metrics import r2_score, mean_squared_error\r\n",
        "\r\n",
        "# Let's update the pipeline with Lasso regression model\r\n",
        "lasso_pipeline = Pipeline([\r\n",
        "    ('data_preprocessing', data_preprocessor),\r\n",
        "    ('lasso', Lasso(alpha = 0.001))\r\n",
        "])\r\n",
        "\r\n",
        "lasso_pipeline.fit(X_train[model_features], y_train.values)\r\n",
        "lassoRegressor_val_predictions = lasso_pipeline.predict(X_val[model_features])\r\n",
        "\r\n",
        "print(\"Lasso on Validation: Mean_squared_error: %f,  R_square_score: %f\" % \\\r\n",
        "      (mean_squared_error(y_val, lassoRegressor_val_predictions),r2_score(y_val, lassoRegressor_val_predictions)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lasso on Validation: Mean_squared_error: 0.651244,  R_square_score: 0.359263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzGXVCcSGS4k"
      },
      "source": [
        "**7.4 ElasticNet (Linear Regression with L2 and L1 regularization)**\r\n",
        "\r\n",
        "Let's finally try ElasticNet from Sklearn library, and check the performance on the validation dataset.\r\n",
        "\r\n",
        "Find more details on ElasticNet here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\r\n",
        "\r\n",
        "ElasticNet is tuning model complexity by adding both $L_2$ and $L_1$ penalty scores for complexity to the model's cost function:\r\n",
        "\r\n",
        "$$\\text{C}_{\\text{regularized}}(\\textbf{w}) = \\text{C}(\\textbf{w}) +  0.5*alpha∗(1-\\textit{l1}_{ratio})||\\textbf{w}||_2^2 + alpha∗\\textit{l1}_{ratio}∗||\\textbf{w}||_1$$\r\n",
        "and using two parameters, $alpha$ and $\\textit{l1}_{ratio}$, to control the strength of the regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DLsS2QSGXq7",
        "outputId": "8cab2874-6355-4eaf-c838-eee42178dccf"
      },
      "source": [
        "from sklearn.linear_model import ElasticNet\r\n",
        "from sklearn.metrics import r2_score, mean_squared_error\r\n",
        "\r\n",
        "# Let's update the pipeline with ElasticNet regression model\r\n",
        "elastic_net_pipeline = Pipeline([\r\n",
        "    ('data_preprocessing', data_preprocessor),\r\n",
        "    ('elastic_net', ElasticNet(alpha = 0.001, l1_ratio = 0.1))\r\n",
        "])\r\n",
        "\r\n",
        "elastic_net_pipeline.fit(X_train[model_features], y_train.values)\r\n",
        "enRegressor_val_predictions = elastic_net_pipeline.predict(X_val[model_features])\r\n",
        "\r\n",
        "print(\"ElasticNet on Validation: Mean_squared_error: %f,  R_square_score: %f\" % \\\r\n",
        "      (mean_squared_error(y_val, enRegressor_val_predictions),r2_score(y_val, enRegressor_val_predictions)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ElasticNet on Validation: Mean_squared_error: 0.648996,  R_square_score: 0.361474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPHTN6jIGZaM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}