{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLA-NLP-Lecture3-Final-Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMEMHM+EA2WH0go8uSA5YwF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LxYuan0420/aws-machine-learning-university-accelerated-nlp/blob/master/colab_notebooks/MLA_NLP_Lecture3_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT4i6ZkarplY",
        "outputId": "da296e5c-99f1-446d-d72a-02ba1bcb0eef"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/gdrive')\r\n",
        "\r\n",
        "%cd \"/gdrive/MyDrive/Colab Notebooks/git/aws-machine-learning-university-accelerated-nlp/colab_notebooks\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/MyDrive/Colab Notebooks/git/aws-machine-learning-university-accelerated-nlp/colab_notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRbxvhdyr0PI"
      },
      "source": [
        "\r\n",
        "**Machine Learning Accelerator - Natural Language Processing - Lecture 3**\r\n",
        "\r\n",
        "**Final Project: Neural Networks and Recurrent Neural Networks (RNNs) for the IMDB Movie Review Dataset**\r\n",
        "\r\n",
        "Dataset: Sentiment (positive or negative) analysis of movie reviews. The dataset is originally hosted here: http://ai.stanford.edu/~amaas/data/sentiment/\r\n",
        "\r\n",
        "We continue to work on our final project dataset. This time, you will try to see how Neural Networks, Recurrent Neural Networks (RNNs), its variants: GRU and LSTM work in predicting the sentiment of review texts. If you are interested in trying Transformers, here is a good place for that too!\r\n",
        "\r\n",
        "Use the notebooks from the class and implement the model, train and test with the corresponding datasets. You can follow these steps:\r\n",
        "\r\n",
        "1. Read training-test data (Given)\r\n",
        "2. Train a classifier (Implement)\r\n",
        "3. Make predictions on your test dataset (Implement)\r\n",
        "\r\n",
        "**1. Reading the dataset**\r\n",
        "\r\n",
        "We will use the pandas library to read our dataset.\r\n",
        "\r\n",
        "Training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goI-6tb_sC72"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "train_df = pd.read_csv('../data/final_project/imdb_train.csv', header=0)\r\n",
        "test_df = pd.read_csv('../data/final_project/imdb_test.csv', header=0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "W_jmKacNxr-H",
        "outputId": "c42caca6-2d26-474e-e3b5-8fbaef592e9d"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This movie makes me want to throw up every tim...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Listening to the director's commentary confirm...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>One of the best Tarzan films is also one of it...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Valentine is now one of my favorite slasher fi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>No mention if Ann Rivers Siddons adapted the m...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  This movie makes me want to throw up every tim...      0\n",
              "1  Listening to the director's commentary confirm...      0\n",
              "2  One of the best Tarzan films is also one of it...      1\n",
              "3  Valentine is now one of my favorite slasher fi...      1\n",
              "4  No mention if Ann Rivers Siddons adapted the m...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv63x-sf54b8"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGIML_GLsHPR"
      },
      "source": [
        "**2. Train a Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ziei9HNsKWG"
      },
      "source": [
        "import torch\r\n",
        "from torchtext.data.utils import get_tokenizer\r\n",
        "from collections import Counter\r\n",
        "from torchtext.vocab import Vocab\r\n",
        "import re"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWG8yacpxden"
      },
      "source": [
        "counter = Counter()\r\n",
        "tokenizer = get_tokenizer(\"basic_english\")\r\n",
        "\r\n",
        "for train_sample in train_df[\"text\"].tolist():\r\n",
        "    counter.update(tokenizer(train_sample))\r\n",
        "\r\n",
        "vocab = Vocab(counter, min_freq=5)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONFF8p5mwDr2"
      },
      "source": [
        "train_text = train_df['text'].values\r\n",
        "train_label = train_df['label'].values\r\n",
        "test_text = test_df['text'].values\r\n",
        "test_label = test_df['label'].values\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZR7C4bKyJJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d797a7-1575-406e-aa0f-0a83111b3d1b"
      },
      "source": [
        "#text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\r\n",
        "from collections import Counter\r\n",
        "import nltk, torchtext\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "nltk.download(\"punkt\")\r\n",
        "\r\n",
        "\r\n",
        "def cleanStr(text):\r\n",
        "    if isinstance(text, str) == False:\r\n",
        "        text = \"\"\r\n",
        "\r\n",
        "    text = text.lower().strip()\r\n",
        "    text = re.sub(\"\\s+\", \" \", text)\r\n",
        "    text = re.compile(\"<.*?>\").sub(\"\", text)\r\n",
        "    return text\r\n",
        "\r\n",
        "def tokenize(text):\r\n",
        "    tokens = []\r\n",
        "    text = cleanStr(text)\r\n",
        "    words = word_tokenize(text)\r\n",
        "    for word in words:\r\n",
        "        tokens.append(word)\r\n",
        "    return tokens\r\n",
        "\r\n",
        "def transformText(text, vocab, max_length):\r\n",
        "    token_arr = torch.zeros((max_length))\r\n",
        "    tokens = tokenize(text)\r\n",
        "\r\n",
        "    if len(tokens) < max_length:\r\n",
        "        tokens += [0]*(max_length - len(tokens))\r\n",
        "\r\n",
        "    tokens = tokens[:max_length]\r\n",
        "    for idx, token in enumerate(tokens):\r\n",
        "        try:\r\n",
        "            token_arr[idx] = vocab.stoi[token]\r\n",
        "        except:\r\n",
        "            token_arr[idx] = 0 \r\n",
        "    return token_arr"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE4j0r2B7YjE",
        "outputId": "01edde26-f107-43b2-976d-53d73af5712f"
      },
      "source": [
        "vocab.stoi[\"hello\"]"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4645"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s14seT-I-v0s"
      },
      "source": [
        "max_length=128\r\n",
        "train_text_transformed = torch.stack([transformText(text, vocab, max_length) for text in train_text])\r\n",
        "test_text_transformed = torch.stack([transformText(text, vocab, max_length) for text in test_text])"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzf8m-tgyZV3"
      },
      "source": [
        "hidden_size = 12\r\n",
        "learning_rate = 0.001\r\n",
        "epochs = 50\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "num_embed = 50\r\n",
        "vocab_size = len(vocab.itos)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyC9AtXlGGRJ"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\r\n",
        "\r\n",
        "train_label = torch.Tensor(train_label)\r\n",
        "test_label = torch.Tensor(test_label)\r\n",
        "train_dataset = TensorDataset(train_text_transformed, train_label)\r\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVf9ktaXDKq1"
      },
      "source": [
        "# define model\r\n",
        "device = torch.device(\"cpu\") # use \"cuda:0\" if you are using GPU\r\n",
        "\r\n",
        "class Net(nn.Module):\r\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers=1):\r\n",
        "        super().__init__()\r\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\r\n",
        "        self.rnn = nn.RNN(embed_size, num_hiddens, num_layers=num_layers)\r\n",
        "        #self.linear = nn.Linear(1536, 1)\r\n",
        "        self.linear = nn.Linear(max_length*num_hiddens, 1) #because in forward() we reshape to (bs, -1)?\r\n",
        "        self.act = nn.Sigmoid()\r\n",
        "\r\n",
        "    def forward(self, inputs):\r\n",
        "        embeddings = self.embedding(inputs)\r\n",
        "        outputs, _ = self.rnn(embeddings)\r\n",
        "        outs = self.linear(outputs.reshape(outputs.shape[0], -1))\r\n",
        "        return self.act(outs)\r\n",
        "    \r\n",
        "model = Net(vocab_size, num_embed, hidden_size).to(device)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ7QlR1AEfIq"
      },
      "source": [
        "trainer = torch.optim.SGD(model.parameters(), lr=learning_rate)\r\n",
        "cross_ent_loss = nn.BCEWithLogitsLoss(reduction='none')"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hbC7lCPAhVG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4811bc2b-e36f-45f8-dd3b-a0361080a4a7"
      },
      "source": [
        "import time\r\n",
        "\r\n",
        "\r\n",
        "for epoch in range(epochs):\r\n",
        "    start = time.time()\r\n",
        "    training_loss=0\r\n",
        "    for idx, (data, target) in enumerate(train_loader):\r\n",
        "        trainer.zero_grad()\r\n",
        "\r\n",
        "        data = data.long().to(device)\r\n",
        "        target = target.to(device)\r\n",
        "\r\n",
        "        output = model(data)\r\n",
        "        L = cross_ent_loss(output.squeeze(1), target).sum()\r\n",
        "        training_loss += L\r\n",
        "\r\n",
        "        L.backward()\r\n",
        "        trainer.step()\r\n",
        "\r\n",
        "    # one epoch finish\r\n",
        "    training_loss /= len(train_label)\r\n",
        "    end = time.time()\r\n",
        "    print(\"Epoch %s. Train_loss %f Seconds %f\" % \\\r\n",
        "          (epoch, training_loss, end-start))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0. Train_loss 0.696218 Seconds 8.009602\n",
            "Epoch 1. Train_loss 0.692315 Seconds 7.897772\n",
            "Epoch 2. Train_loss 0.691051 Seconds 8.006441\n",
            "Epoch 3. Train_loss 0.689180 Seconds 7.946592\n",
            "Epoch 4. Train_loss 0.686496 Seconds 7.878174\n",
            "Epoch 5. Train_loss 0.683239 Seconds 7.861934\n",
            "Epoch 6. Train_loss 0.679714 Seconds 7.829571\n",
            "Epoch 7. Train_loss 0.676184 Seconds 7.870228\n",
            "Epoch 8. Train_loss 0.672821 Seconds 8.107740\n",
            "Epoch 9. Train_loss 0.669692 Seconds 8.202859\n",
            "Epoch 10. Train_loss 0.666817 Seconds 8.062102\n",
            "Epoch 11. Train_loss 0.664194 Seconds 8.053298\n",
            "Epoch 12. Train_loss 0.661803 Seconds 7.959071\n",
            "Epoch 13. Train_loss 0.659608 Seconds 8.040235\n",
            "Epoch 14. Train_loss 0.657575 Seconds 8.090099\n",
            "Epoch 15. Train_loss 0.655677 Seconds 7.996216\n",
            "Epoch 16. Train_loss 0.653892 Seconds 8.021352\n",
            "Epoch 17. Train_loss 0.652199 Seconds 7.986277\n",
            "Epoch 18. Train_loss 0.650590 Seconds 8.012913\n",
            "Epoch 19. Train_loss 0.649056 Seconds 8.063029\n",
            "Epoch 20. Train_loss 0.647595 Seconds 8.055485\n",
            "Epoch 21. Train_loss 0.646201 Seconds 8.003919\n",
            "Epoch 22. Train_loss 0.644870 Seconds 8.179414\n",
            "Epoch 23. Train_loss 0.643597 Seconds 8.059028\n",
            "Epoch 24. Train_loss 0.642377 Seconds 8.063028\n",
            "Epoch 25. Train_loss 0.641206 Seconds 8.022620\n",
            "Epoch 26. Train_loss 0.640081 Seconds 8.010056\n",
            "Epoch 27. Train_loss 0.639000 Seconds 8.004124\n",
            "Epoch 28. Train_loss 0.637959 Seconds 7.815965\n",
            "Epoch 29. Train_loss 0.636956 Seconds 7.854582\n",
            "Epoch 30. Train_loss 0.635990 Seconds 7.935142\n",
            "Epoch 31. Train_loss 0.635057 Seconds 7.953789\n",
            "Epoch 32. Train_loss 0.634155 Seconds 7.928738\n",
            "Epoch 33. Train_loss 0.633281 Seconds 8.044248\n",
            "Epoch 34. Train_loss 0.632433 Seconds 7.993571\n",
            "Epoch 35. Train_loss 0.631607 Seconds 8.056006\n",
            "Epoch 36. Train_loss 0.630802 Seconds 7.925426\n",
            "Epoch 37. Train_loss 0.630015 Seconds 7.915181\n",
            "Epoch 38. Train_loss 0.629245 Seconds 7.931714\n",
            "Epoch 39. Train_loss 0.628490 Seconds 8.004307\n",
            "Epoch 40. Train_loss 0.627750 Seconds 7.958493\n",
            "Epoch 41. Train_loss 0.627022 Seconds 7.965747\n",
            "Epoch 42. Train_loss 0.626308 Seconds 7.866899\n",
            "Epoch 43. Train_loss 0.625606 Seconds 7.923956\n",
            "Epoch 44. Train_loss 0.624918 Seconds 7.966741\n",
            "Epoch 45. Train_loss 0.624243 Seconds 7.894330\n",
            "Epoch 46. Train_loss 0.623581 Seconds 7.955463\n",
            "Epoch 47. Train_loss 0.622934 Seconds 7.960017\n",
            "Epoch 48. Train_loss 0.622300 Seconds 8.009657\n",
            "Epoch 49. Train_loss 0.621681 Seconds 7.884643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvqI-v6vBMlF",
        "outputId": "552ca991-f822-47e3-bcfb-8b490f1582a5"
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\r\n",
        "import numpy as np\r\n",
        "# Get validation predictions\r\n",
        "test_predictions = model(test_text_transformed.to(device).long())\r\n",
        "\r\n",
        "# Round predictions: 1 if pred>0.5, 0 otherwise\r\n",
        "test_predictions = np.round(test_predictions.detach().numpy())\r\n",
        "\r\n",
        "print(\"Classification Report\")\r\n",
        "print(classification_report(test_label.numpy(), test_predictions))\r\n",
        "print(\"Accuracy\")\r\n",
        "print(accuracy_score(test_label.numpy(), test_predictions))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.57      0.85      0.68     12500\n",
            "         1.0       0.70      0.36      0.48     12500\n",
            "\n",
            "    accuracy                           0.61     25000\n",
            "   macro avg       0.64      0.61      0.58     25000\n",
            "weighted avg       0.64      0.61      0.58     25000\n",
            "\n",
            "Accuracy\n",
            "0.60544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxJ8_j5t3_3I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}